# miika
The place to tell your worries.

To run inference.py in server, you need at least 6 GB of GPU RAM.

Run (for now): Go to root, then execute ./run.sh

Figma: https://www.figma.com/file/WSLLnv6wrC8Kz755jaqVzA/chatroom?type=design&node-id=0%3A1&mode=design&t=zGnJ3aCIh5rJK9ax-1